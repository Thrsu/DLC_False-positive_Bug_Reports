from openai import OpenAI
import pandas as pd
import time
import re


client = OpenAI(
    base_url='',
    api_key=''
)

input_file_path = "openvino_issue_with_example.xlsx"
output_file_path = "openvino_fewshot_choice.xlsx"

df = pd.read_excel(input_file_path)

def get_openai_response(content, model="gpt-4o"):
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an expert of OpenVINO (a deep learning compiler)."},
                {"role": "user", "content": content}
            ],
            temperature=0.8
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"OpenAI access fail，retrying: {e}")
        time.sleep(10)
        return None

responses = []
explanations = []

examples = {
    1: """
**Title**: Intel Myriad X, "current Interpolate supports 'nearest' and 'linear' modes only"  
**Description**:  
A user attempts to convert and compile an ONNX model (`efficientvit-sam l0_decoder.onnx`) to run on an Intel Myriad X (VPU) using the Luxonis blob converter (OpenVINO 2022.1 backend). The process fails with an error indicating the use of an unsupported interpolation mode (`cubic`). Later investigation by an OpenVINO contributor revealed that the model also contains other unsupported operations like `Sin`, which are not available on the Myriad plugin. These issues are due to the limitations of the target hardware (Myriad X), not a defect in OpenVINO itself.
```
FalsePositive_Probability: 0.95

Reasoning: The failure is caused by the use of ONNX operators (`cubic` interpolation, `Sin`) that are unsupported on the target hardware (Myriad X VPU). OpenVINO correctly rejects models with unsupported operations, and the issue lies in hardware compatibility, not a bug in the compiler implementation.
```
""",
    2: """
**Title**: [Build]: failed to parse GPG signature for RPM repository  
**Description**:  
When trying to install OpenVINO 2024.1.0 on Fedora 40 using the official Yum repository, installation fails due to a GPG signature verification error. The error message indicates that the signature was generated by a non-conformant OpenPGP implementation and cannot be parsed. Disabling `gpgcheck` and `repo_gpgcheck` allows installation to proceed but compromises security. The problem was not reproducible on Fedora 38 but was later confirmed and fixed by OpenVINO maintainers, who updated the repository signing mechanism to comply with the newer verification rules introduced in Fedora 40.
```
FalsePositive_Probability: 0.05

Reasoning: The issue stems from OpenVINO's packaging process generating a non-standard GPG signature, which breaks compatibility with Fedora 40's stricter verification. This is a bug in the OpenVINO distribution process, not user misconfiguration or misuse.
```
""",
    3: """
**Title**: [Bug]: Error when changing the batch size of YOLOv8-seg from Ultralytics
**Description**:
When attempting to run inference on a YOLOv8-seg model (exported using the Ultralytics API) using `benchmark_app` with a changed batch size (`-shape [2,3,640,640]`), OpenVINO 2022.3 fails with the following reshape error during model validation:

```
While validating node 'opset1::Reshape /model.22/dfl/Reshape':
Requested output shape [1,4,16,8400] is incompatible with input shape [1,64,16800]
```

The user traced the issue to a likely hardcoded reshape layer in the model IR that doesn’t adjust correctly when the batch size is modified. Initial interpretation might treat this as a bug in OpenVINO's reshaping logic, but the OpenVINO team clarified that the actual cause lies in **how the ONNX model is exported** using Ultralytics.

By default, the Ultralytics API exports models using static tracing, which applies constant folding to reshape layers. This leads to **non-reshapable models**, because the output shape is hardcoded during ONNX export. To preserve reshapability, the export must be done with `dynamic=True`, which keeps input/output dimensions flexible.

Thus, the problem is not in OpenVINO, but in the user ONNX export configuration.

```
FalsePositive_Probability: 0.95

Reasoning:
This issue stems from improper ONNX model export using static tracing, which results in constant folding and hardcoded reshape dimensions. The OpenVINO reshape failure is a natural consequence of these fixed dimensions. When exported correctly with `dynamic=True`, the model becomes reshapable, and the issue disappears. This makes it a **user-side configuration problem**, not a bug in OpenVINO itself.
```
""",
    4: """
**Title**: [Bug]: Conversion Failure for `torch.nn.BatchNorm1d` due to input element type mismatch
**Description**:
When converting a TorchScript module containing `torch.nn.BatchNorm1d` to OpenVINO IR using `ov.convert_model`, the conversion fails with an `OpConversionFailure`. The root cause is a mismatch between the input tensor's data type (`float16`) and the BatchNorm weights, biases, running mean, and running variance, which are in `float32`. OpenVINO's `BatchNormInference` operator expects all inputs to have the same element type. This results in the following error: Input element types do not match

Despite appearing to be a user error due to the atypical input shape `[0, 97, 40]`, the actual issue lies in OpenVINO's lack of type promotion or casting for mixed-precision inputs during the conversion process. The OpenVINO team later confirmed the problem and fixed it by enabling `BatchNormInference` to support inputs of any float type. This confirms the presence of a real bug in OpenVINO’s conversion logic rather than user misuse.

```
FalsePositive_Probability: 0.05

Reasoning:
Although the input tensor shape `[0, 97, 40]` might raise suspicion of a user error, the failure occurs due to OpenVINO's inability to handle valid mixed-precision input combinations during model conversion. The OpenVINO maintainers acknowledged the issue and addressed it in the framework’s internals, validating this as a true bug rather than a false positive.
```
""",
    5: """
**Title**: [Bug]: Using Benchmark tool from different terminals decreases the inference performance
**Description**:
A user observed that when running multiple instances of the `benchmark_app` tool from different terminals (i.e., multiple processes), the total throughput (FPS) for YOLOv8 model inference on a GPU decreases significantly. When only one instance is run, the FPS is \~194.66, but when running two instances simultaneously, the FPS drops to \~60 per instance. Similar performance degradation was also observed with CPU inference using custom OpenVINO scripts.

```
FalsePositive_Probability: 0.95

Reasoning:
The performance drop is due to **expected system-level resource contention and OpenVINO's default CPU affinity settings**, not due to an internal error or regression. Once proper inference configuration (`"AFFINITY": "NUMA"`) is applied, the issue resolves. Therefore, the user report, while valid in observation, is **not a bug in OpenVINO** and stems from default behavior in multi-process inference environments.
```
""",
    6: """
**Title**: [Bug]: Failed to find reference implementation for `Range` Layer with Range Type on constant propagation
**Description**:
When compiling a Torch-to-ONNX converted model (HITNet) to OpenVINO IR using the Model Optimizer and then targeting the `MYRIAD` device with the `compile_tool`, the compilation fails with the error:

```
Failed to find reference implementation for /model/level.4/init/Range Layer with Range Type on constant propagation
```

This issue does **not** occur when running the same IR model on the CPU device, indicating a discrepancy in backend support or behavior. Initial analysis might suggest the `Range` operation is simply unsupported on MYRIAD, but the actual root cause lies in OpenVINO's **constant propagation pass**, which attempts to fold a `Range` operation during compilation, even though no backend implementation exists for this specific usage.

The issue was solved by disabling constant folding** for branches containing `Range` operations, confirming that the issue was due to incorrect handling during optimization rather than a hardware limitation or user misconfiguration.

```
FalsePositive_Probability: 0.05

Reasoning:
Although the failure only surfaces when compiling for MYRIAD, and the model runs fine on CPU, the root cause is **not** hardware incompatibility but a bug in OpenVINO’s optimization pass that attempts to propagate constants through a `Range` operation lacking a supported reference implementation. It is a valid bug in the compiler logic, not expected behavior or user misuse.
```
""",
    7: """
**Title**: [Bug]: Not the same results on CPU and GPU

**Description**:
A user reported that after upgrading from OpenVINO version 2023.\* to 2024.1.0, the inference results of a custom classifier model became completely different when executed on CPU versus GPU. Even when the model was optimized using the 2023 version, using OpenVINO 2024 for inference led to large discrepancies between CPU and GPU outputs. This behavior persisted even when the network was reduced to a single convolutional layer, showing that the inconsistency was fundamental and not model-specific.

The CPU consistently produced results in line with expectations, while GPU output significantly diverged—not by small epsilon-level differences, but in a completely different output distribution. The issue also remained present in OpenVINO 2024.2.0.

```
FalsePositive_Probability: 0.95

Reasoning:
The discrepancy arises not from a bug in OpenVINO, but from **precision mismatch across devices**. By default, CPU inference uses FP32 while GPU uses FP16 to optimize performance. This leads to divergent intermediate computations and final outputs. Once the inference precision on GPU is explicitly set to match the CPU (FP32), the results become consistent. Therefore, the issue is **not a bug**, but a misunderstanding of default precision behavior in OpenVINO 2024.
```
""",
    8: """
**Title**: [Bug]: Import failure due to unsupported `SnippetsOpset` in exported model
**Description**:
When using OpenVINO 2022.1 to export and subsequently re-import a compiled model using either the old (`InferenceEngine::Core`) or new (`ov::Core`) API, the process fails for certain models—such as `poconetlike`—with the following error:

```
ir_deserializer.cpp:715 Cannot create Subgraph layer Multiply_289158 id:68 from unsupported opset: SnippetsOpset.
```

While the same code successfully exports and imports models like `vehicle-detection`, the `poconetlike` model triggers a failure during the `import_model()` or `ImportNetwork()` step due to the presence of `SnippetsOpset`, an internal opset not correctly handled by the OpenVINO importer at the time.

This is not expected behavior** and acknowledged it as a bug in the IR deserialization logic. A proper fix was required to correctly handle models containing ops from `SnippetsOpset` during model import.

```
FalsePositive_Probability**: 0.05

Reasoning:
The bug arises from the internal handling of subgraphs and opset registration during model import, not user error or version misuse. It is a real bug, not a false positive.
```
"""
}

for i, row in df.iterrows():
    title = row["Title"]
    body = row["Body"]
    fp_id = row["FP_Example"]
    bug_id = row["Bug_Example"]
    fp_example = examples.get(fp_id, "")
    bug_example = examples.get(bug_id, "")
    
    content = f"""
You are assisting in the triage of issue reports submitted by users of the deep learning compiler **OpenVINO**. Each issue report contains a **Title** and a **Description**.

Your task is to analyze the issue and estimate the likelihood that the issue is a **FalsePositive** — meaning the problem is **not caused by a bug in the compiler**, but rather due to incorrect usage, invalid input, user environment misconfiguration, or misunderstanding of expected behavior. In contrast, if a issue is not a FalsePositive bug report, then it is a genuine bug in the deep learning compiler which was introduced by the compiler developers and must be fixed by modifying the compiler’s source code.

**Do not directly classify the issue. Instead, output the estimated probability (between 0.0 and 1.0) that the issue is a FalsePositive.**

---

### Output Format

Please output your response in **exactly** the following format:

```
FalsePositive_Probability: <Float between 0.0 and 1.0>

Reasoning: <Brief explanation referencing the likely root cause and your rationale>
```

---

### Important Notes
- Pay special attention to whether the error occurs in code or scripts maintained by OpenVINO: If the problematic line or configuration appears in OpenVINO’s official repository (e.g., scripts/setupvars.bat, cmake, .cpp, .py files), it is not a user mistake

- **Environment misconfiguration, or compatibility issues should only be considered FalsePositive if there's strong evidence the issue is entirely on the user's side.**  
For example, a user setting an invalid path or missing dependency they were explicitly told to install.  
- However, if the misconfiguration is due to:
  - unclear documentation,
  - silently failing logic in OpenVINO,
  - breaking changes introduced without version check,
  - or dependency incompatibility caused by OpenVINO’s own setup/installation script,

then it is **not** a FalsePositive. It should be treated as a genuine bug in the compiler (FalsePositive_Probability closer to 0.0).

- When unsure whether an error is caused by the user or the system, **lean toward classifying it as TVM’s responsibility** unless there's direct evidence of user misuse.

---

## Examples
### Example 1:
{fp_example}

### Example 2:
{bug_example}
---

## Output Format

Please output your response in **exactly** the following format:

```
FalsePositive_Probability: <Float between 0.0 and 1.0>

Reasoning: <Brief explanation referencing the likely root cause and your rationale>
```

---
# Use this format to classify the issue below:
# - **Title**: {title}
# - **Description**: {body}
"""
    response = get_openai_response(content)
    explanations.append(response)

    confidence = None
    reasoning = ""

    if response:
        confidence_match = re.search(r"FalsePositive_Probability:\s*([0-9]*\.?[0-9]+)", response)
        reasoning_match = re.search(r"Reasoning:\s*(.+)", response, re.IGNORECASE | re.DOTALL)

        if confidence_match:
            confidence = confidence_match.group(1)
        if reasoning_match:
            reasoning = reasoning_match.group(1).strip()

    df.at[i, "FalsePositive_Probability"] = confidence
    df.at[i, "Reasoning"] = reasoning

    df.at[i, "Explanation"] = explanations[-1]

    df.to_excel(output_file_path, index=False)
    time.sleep(3)